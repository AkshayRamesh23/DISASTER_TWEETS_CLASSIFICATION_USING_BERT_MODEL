{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKOTlwcmxmej"
      },
      "source": [
        "## BERT for Disaster Tweets Classification (Kaggle)\n",
        "\n",
        "by Akshay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMnjI7luHO8I",
        "outputId": "85185802-9302-45ff-828c-c4bda01d332f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgluAffQd68e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "297badf5-4433-44f3-dc58-6ae65aa57430"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov  9 01:08:01 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX_ZDhicpHkV"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSU7yERLP_66"
      },
      "source": [
        "## 1.1. Check for Colab GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEfSbAA4QHas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85fd5b0a-ee4d-4c68-dffd-1eccbbf9e055"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYsV4H8fCpZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f490c637-876e-4bb5-f406-fd189c19ed24"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ElsnSNUridI"
      },
      "source": [
        "## 1.2. Install transformers Library\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if a GPU is available, and if not, use the CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "Ey9g6CrlFjtR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmMdkZO8R6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf6c212-4c3a-497d-8830-bcd4aa60028d"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guw6ZNtaswKc"
      },
      "source": [
        "# 2. Load Disaster Tweets Dataset (from Kaggle)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-QAg5iz1Cly"
      },
      "source": [
        "## 2.1 Load csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UkeC7SG2krJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "78739770-a9cf-486f-bf80-74dd41164dca"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "print('Number of training sentences: ', len(df))\n",
        "df.sample(5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences:  7613\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id     keyword       location  \\\n",
              "3471  4966   explosion            NaN   \n",
              "2510  3604  desolation       New York   \n",
              "2534  3638  desolation            NaN   \n",
              "3606  5148       fatal  Also follow ?   \n",
              "1781  2556       crash      Tennessee   \n",
              "\n",
              "                                                   text  target  \n",
              "3471  Report: Corey Brewer was drug tested after 51-...       1  \n",
              "2510  The Hobbit Desolation of Smaug Thranduil 4' sc...       0  \n",
              "2534  Escape The Heat (and the #ORShow) for a trail ...       0  \n",
              "3606  Boy 11 charged with manslaughter in shooting d...       1  \n",
              "1781  Just bought another @meinlcymbals 18' medium c...       0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8f8ac3e4-5a8f-406b-bc5c-5e072c8917d5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3471</th>\n",
              "      <td>4966</td>\n",
              "      <td>explosion</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Report: Corey Brewer was drug tested after 51-...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2510</th>\n",
              "      <td>3604</td>\n",
              "      <td>desolation</td>\n",
              "      <td>New York</td>\n",
              "      <td>The Hobbit Desolation of Smaug Thranduil 4' sc...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2534</th>\n",
              "      <td>3638</td>\n",
              "      <td>desolation</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Escape The Heat (and the #ORShow) for a trail ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3606</th>\n",
              "      <td>5148</td>\n",
              "      <td>fatal</td>\n",
              "      <td>Also follow ?</td>\n",
              "      <td>Boy 11 charged with manslaughter in shooting d...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1781</th>\n",
              "      <td>2556</td>\n",
              "      <td>crash</td>\n",
              "      <td>Tennessee</td>\n",
              "      <td>Just bought another @meinlcymbals 18' medium c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f8ac3e4-5a8f-406b-bc5c-5e072c8917d5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8f8ac3e4-5a8f-406b-bc5c-5e072c8917d5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8f8ac3e4-5a8f-406b-bc5c-5e072c8917d5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-51c2945a-ff15-4ccb-b82e-f3c878d5e8a0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-51c2945a-ff15-4ccb-b82e-f3c878d5e8a0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-51c2945a-ff15-4ccb-b82e-f3c878d5e8a0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfl3uMImt4El",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b73030b-9faa-41eb-d4eb-e2fb1d21b575"
      },
      "source": [
        "# Print some negative sample tweets\n",
        "for txt in df[df.target==0].text.sample(5).values:\n",
        "  print(txt)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China's Stock Market Crash: Are There Gems In The Rubble? http://t.co/3PBFyJx0yA\n",
            "A Rocket To The Moon  ? Sleeping With Sirens ?A Rocket To The Moon ????????????\n",
            "@ArianaGrande @justinbieber All the loves be screaming at this one ??????\n",
            "@_rosewell it has crashed so many times the past couple hours\n",
            "infected bloody ear piercings are always fun??\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKElcfFeuUy0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c40492-ef5e-4390-afc2-d0d7a061ead8"
      },
      "source": [
        "# Print some positive sample tweets\n",
        "for txt in df[df.target==1].text.sample(5).values:\n",
        "  print(txt)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#hot  C-130 specially modified to land in a stadium and rescue hostages in Iran in 1980 http://t.co/wpGvAyfkBQ #prebreak #best\n",
            "Bomb head? Explosive decisions dat produced more dead children than dead bodies trapped tween buildings on that day in September there\n",
            "when a monster truck racer catches on fire at the fair\n",
            "'Food crematoria' in Russia provoke outrage amid crisis famine memories\n",
            "http://t.co/FelR5a1hBP\n",
            "Twelve feared killed in Pakistani air ambulance helicopter crash http://t.co/3bRme6Sn4t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IVrgmW8vKAD"
      },
      "source": [
        "Looks like many of positive tweets are coming from a more formal source."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blqIvQaQncdJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2f04fce-069b-4374-d7d3-a89350197e02"
      },
      "source": [
        "df.text.isna().sum()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XnHFjfv2mis"
      },
      "source": [
        "There is no empty cell for text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j795oV7b2JFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16aeb005-152f-47a9-a86b-6012d1495566"
      },
      "source": [
        "print(\"Positive data: {:.2f}%\".format(len(df[df.target==1])*100/len(df)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive data: 42.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5ABhGm5xBvl"
      },
      "source": [
        "Ok, we have a pretty balanced dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuE5BqICAne2"
      },
      "source": [
        "tweets = df.text.values\n",
        "labels = df.target.values"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXuqsaul1HhT"
      },
      "source": [
        "## 2.2  http://... URLs in tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwwPJd1zv7Ps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec3486d-af8c-4517-e802-c6ec18113546"
      },
      "source": [
        "print(\"{} out of {} tweets have a http://... link within itself. ({:.2f}%)\".format(len([t for t in tweets if \"http://\" in t]), len(df), len([t for t in tweets if \"http://\" in t])*100/len(df)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3604 out of 7613 tweets have a http://... link within itself. (47.34%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a30mP7hB0Vy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15137b74-7d4b-418a-acc3-61d5f99d8f1a"
      },
      "source": [
        "[t for t in tweets if \"http://\" in t][:5]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C',\n",
              " 'We always try to bring the heavy. #metal #RT http://t.co/YAo1e0xngw',\n",
              " '#AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi',\n",
              " 'On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE http://t.co/qqsmshaJ3N',\n",
              " 'INEC Office in Abia Set Ablaze - http://t.co/3ImaomknnA']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vctexe1L8rI1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66f5e6f9-e623-46b6-a254-0f83a1b0c18a"
      },
      "source": [
        "# Print some tweets with URL that does NOT have URL at the end\n",
        "[t for t in [t for t in tweets if \"http://\" in t] if \"http://\" not in t.split()[-1]][:5]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Check these out: http://t.co/rOI2NSmEJJ http://t.co/3Tj8ZjiN21 http://t.co/YDUiXEfIpE http://t.co/LxTjc87KLS #nsfw',\n",
              " 'Check these out: http://t.co/rOI2NSmEJJ http://t.co/3Tj8ZjiN21 http://t.co/YDUiXEfIpE http://t.co/LxTjc87KLS #nsfw',\n",
              " '#NowPlaying: Rene Ablaze &amp; Ian Buff - Magnitude http://t.co/Av2JSjfFtc  #EDM',\n",
              " 'http://t.co/GKYe6gjTk5 Had a #personalinjury accident this summer? Read our advice &amp; see how a #solicitor can help #OtleyHour',\n",
              " '&gt;&gt; $15 Aftershock : Protect Yourself and Profit in the Next Global Financial... ##book http://t.co/f6ntUc734Z\\n@esquireattire']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9XMBahi1YTU"
      },
      "source": [
        "Let's see if having a URL makes the tweet more probable to be a disaster tweet (target=1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_nFVYdd1k2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "199be4fe-ca56-4670-f5b7-bd53e748a9d8"
      },
      "source": [
        "print(\"percentage of POSITIVE samples containing http URLs at the end: {:.2f}%\".format(len([t for t in df[df['target']==1]['text'] if \"http://\" in t])*100/len(df[df['target']==1])))\n",
        "print(\"percentage of NEGATIVE samples containing http URLs at the end: {:.2f}%\".format(len([t for t in df[df['target']==0]['text'] if \"http://\" in t])*100/len(df[df['target']==0])))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "percentage of POSITIVE samples containing http URLs at the end: 62.86%\n",
            "percentage of NEGATIVE samples containing http URLs at the end: 35.65%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wGiW_-76BrV"
      },
      "source": [
        "Ok, seems like positive samples have higher probablity of having a URL. Maybe it's because to share/announce a disaster, one might share a news/youtube link as a source."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSq8PLp21RIa"
      },
      "source": [
        "## 2.3 @user_id tags in tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6b05_6a0ugG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f9ed523-1b25-4db2-a1f7-c7762cfd81c1"
      },
      "source": [
        "print(\"{} out of {} tweets have a @user_id tag within itself. ({:.2f}%)\".format(len([t for t in tweets if \"@\" in t]), len(df), len([t for t in tweets if \"@\" in t])*100/len(df)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2039 out of 7613 tweets have a @user_id tag within itself. (26.78%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQgvOvXb0eEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca8e5787-71a3-49aa-fae8-5c8957cb37fc"
      },
      "source": [
        "[t for t in tweets if \"@\" in t][:5]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C',\n",
              " \"@PhDSquares #mufc they've built so much hype around new acquisitions but I doubt they will set the EPL ablaze this season.\",\n",
              " 'SOOOO PUMPED FOR ABLAZE ???? @southridgelife',\n",
              " \"Noches El-Bestia '@Alexis_Sanchez: happy to see my teammates and training hard ?? goodnight gunners.?????? http://t.co/uc4j4jHvGR'\",\n",
              " 'Set our hearts ablaze and every city was a gift And every skyline was like a kiss upon the lips @\\x89Û_ https://t.co/cYoMPZ1A0Z']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtkfZyDL3qKT"
      },
      "source": [
        "Let's see if having id tag(s) makes the tweet more probable to be a disaster tweet (target=1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KgJ9qP63mCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bce480c3-31d0-4a3e-8621-5b72975ef38c"
      },
      "source": [
        "print(\"percentage of POSITIVE samples containing @user_id tag: {:.2f}%\".format(len([t for t in df[df['target']==1]['text'] if \"@\" in t])*100/len(df[df['target']==1])))\n",
        "print(\"percentage of NEGATIVE samples containing @user_id tag: {:.2f}%\".format(len([t for t in df[df['target']==0]['text'] if \"@\" in t])*100/len(df[df['target']==0])))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "percentage of POSITIVE samples containing @user_id tag: 20.67%\n",
            "percentage of NEGATIVE samples containing @user_id tag: 31.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40pFQ-CJ1lsW"
      },
      "source": [
        "## 2.3 Hashtags in tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4YGWZrwP3vK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "099366bd-3c65-47b5-de77-f794d77f24de"
      },
      "source": [
        "print(\"{} out of {} tweets have a # tag within itself. ({:.2f}%)\".format(len([t for t in tweets if \"#\" in t]), len(df), len([t for t in tweets if \"#\" in t])*100/len(df)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1761 out of 7613 tweets have a # tag within itself. (23.13%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URNNZ1A3QQMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a035b28f-44bd-49a1-bb7c-a271836a70c3"
      },
      "source": [
        "print(\"percentage of POSITIVE samples containing # tag: {:.2f}%\".format(len([t for t in df[df['target']==1]['text'] if \"#\" in t])*100/len(df[df['target']==1])))\n",
        "print(\"percentage of NEGATIVE samples containing # tag: {:.2f}%\".format(len([t for t in df[df['target']==0]['text'] if \"#\" in t])*100/len(df[df['target']==0])))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "percentage of POSITIVE samples containing # tag: 26.75%\n",
            "percentage of NEGATIVE samples containing # tag: 20.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex5O1eV-Pfct"
      },
      "source": [
        "# 3. Tokenize\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8kEDRvShcU5"
      },
      "source": [
        "## 3.1. BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z474sSC6oe7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb892be4-d490-43ee-b63b-17d729e3b12b"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLIbudgfh6F0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89986906-4ff5-42ae-faf7-f1b21ecd16f1"
      },
      "source": [
        "print(' Original: ', tweets[1], labels[1])\n",
        "print('Tokenized: ', tokenizer.tokenize(tweets[1]))\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[1])))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original:  Forest fire near La Ronge Sask. Canada 1\n",
            "Tokenized:  ['forest', 'fire', 'near', 'la', 'ron', '##ge', 'sas', '##k', '.', 'canada']\n",
            "Token IDs:  [3224, 2543, 2379, 2474, 6902, 3351, 21871, 2243, 1012, 2710]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D55dBSx53yz"
      },
      "source": [
        "## 3.2 http:// URLs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqqhFdvcwVBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23fa01e1-837a-4e5f-e536-992491056c0f"
      },
      "source": [
        "print(' Original: ', tweets[-1]) # a tweet with http URL\n",
        "print('   Target: ', labels[-1])\n",
        "print('Tokenized: ', tokenizer.tokenize(tweets[-1]))\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[-1])))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original:  The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d\n",
            "   Target:  1\n",
            "Tokenized:  ['the', 'latest', ':', 'more', 'homes', 'ra', '##zed', 'by', 'northern', 'california', 'wild', '##fire', '-', 'abc', 'news', 'http', ':', '/', '/', 't', '.', 'co', '/', 'y', '##my', '##4', '##rsk', '##q', '##3d']\n",
            "Token IDs:  [1996, 6745, 1024, 2062, 5014, 10958, 5422, 2011, 2642, 2662, 3748, 10273, 1011, 5925, 2739, 8299, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1061, 8029, 2549, 27472, 4160, 29097]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHGjUmee8CIs"
      },
      "source": [
        "Random alphabetical tokens from URLs are useless. We shold just keep \"http\" token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e9bjCIZ8AoV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "840daf61-f10e-455a-c468-3acb5899fe26"
      },
      "source": [
        "tweets = [\" \".join([word if 'http://' not in word else \"http\" for word in t.split()]) for t in tweets]\n",
        "tweets[-1]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Latest: More Homes Razed by Northern California Wildfire - ABC News http'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skZAop2I57t8"
      },
      "source": [
        "## 3.3 @user_id mentions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIfajyJf4Rej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ddaba39-5eab-4f4e-afdd-d2e077a5c242"
      },
      "source": [
        "print(' Original: ', tweets[-4])\n",
        "print('   Target: ', labels[-4])\n",
        "print('Tokenized: ', tokenizer.tokenize(tweets[-4]))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original:  @aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part of the state. Very troubling.\n",
            "   Target:  1\n",
            "Tokenized:  ['@', 'aria', '_', 'ah', '##rar', '##y', '@', 'theta', '##wn', '##iest', 'the', 'out', 'of', 'control', 'wild', 'fires', 'in', 'california', 'even', 'in', 'the', 'northern', 'part', 'of', 'the', 'state', '.', 'very', 'tr', '##ou', '##bling', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xL1HDYP4kqz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8520af24-b509-4d67-e9ea-ae69570bbd43"
      },
      "source": [
        "print(' Original: ', tweets[-17])\n",
        "print('   Target: ', labels[-17])\n",
        "print('Tokenized: ', tokenizer.tokenize(tweets[-17]))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original:  RT @LivingSafely: #NWS issues Severe #Thunderstorm Warnings for parts of #AR #NC #OK. Expect more trauma cases: http\n",
            "   Target:  1\n",
            "Tokenized:  ['rt', '@', 'living', '##sa', '##fe', '##ly', ':', '#', 'nw', '##s', 'issues', 'severe', '#', 'thunder', '##storm', 'warnings', 'for', 'parts', 'of', '#', 'ar', '#', 'nc', '#', 'ok', '.', 'expect', 'more', 'trauma', 'cases', ':', 'http']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHch2zUi5AkX"
      },
      "source": [
        "Even with user_id w/ common nouns like above instead of proper nouns like names, resulting tokens seem not reasonable due to spacing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9uTAipf4q3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be2a2880-7af7-427e-9b84-576322a880f0"
      },
      "source": [
        "tokenizer.tokenize(\"Living safely\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['living', 'safely']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUbt3PylxMub"
      },
      "source": [
        "Let's keep \"@\" token only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK4K8zYjHM-Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8c48bfdf-5667-41dc-8ea9-efc944d270d7"
      },
      "source": [
        "tweets = [\" \".join([word if '@' not in word else \"@\" for word in t.split()]) for t in tweets]\n",
        "tweets[-4]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'@ @ The out of control wild fires in California even in the Northern part of the state. Very troubling.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6w8elb-58GJ"
      },
      "source": [
        "## 3.4 Tokenize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvP934rdFSvg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be7a2614-c4a6-4aa9-a329-87bb29e2d118"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "encoded_tweets = [tokenizer.encode(t) for t in tweets]\n",
        "lens = np.array([len(t) for t in encoded_tweets])\n",
        "\n",
        "print('# of sentences:', len(tweets))\n",
        "print('Max sentence length: ', max(lens))\n",
        "print('Avg sentence length: ', np.mean(lens))\n",
        "print('Median sentence length: ', np.median(lens))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of sentences: 7613\n",
            "Max sentence length:  80\n",
            "Avg sentence length:  23.727702613949823\n",
            "Median sentence length:  24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M296yz577fV"
      },
      "source": [
        "Just in case there are some longer test sentences, I'll set the maximum length to 100.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bBdb3pt8LuQ"
      },
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "def encode(sentences, labels, tokenizer, max_len):\n",
        "    encoded_dicts = [tokenizer.encode_plus(\n",
        "                            sent,                      # Sentence to encode.\n",
        "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                            max_length = max_len,           # Pad & truncate all sentences.\n",
        "                            pad_to_max_length = True,\n",
        "                            return_attention_mask = True,   # Construct attn. masks.\n",
        "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                     ) for sent in sentences]\n",
        "    input_ids = [d['input_ids'] for d in encoded_dicts]\n",
        "    attention_masks = [d['attention_mask'] for d in encoded_dicts]\n",
        "\n",
        "    # Convert the lists into tensors.\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return input_ids, attention_masks, labels"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-eDLAn17ESW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4421abc8-48e7-4ccd-cbda-00d31becf339"
      },
      "source": [
        "input_ids, attention_masks, labels = encode(tweets, labels, tokenizer, max_len=100)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIEXgcijQbd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33d06eb7-aabc-4232-fed6-ec822f6f802b"
      },
      "source": [
        "tokenizer.convert_ids_to_tokens(input_ids[0][:20])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'our',\n",
              " 'deeds',\n",
              " 'are',\n",
              " 'the',\n",
              " 'reason',\n",
              " 'of',\n",
              " 'this',\n",
              " '#',\n",
              " 'earthquake',\n",
              " 'may',\n",
              " 'allah',\n",
              " 'forgive',\n",
              " 'us',\n",
              " 'all',\n",
              " '[SEP]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRp4O7D295d_"
      },
      "source": [
        "## 3.5 Build Train & Validation DatatLoaders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEgLpFVlo1Z-"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "def make_dataloader(input_ids, attention_masks, labels, split=1):\n",
        "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "    if split:\n",
        "        train_size = int(0.9 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "        print('{} training samples'.format(train_size))\n",
        "        print('{} validation samples'.format(val_size))\n",
        "    else:\n",
        "        train_dataset = dataset\n",
        "        print(print('{} training samples (no validation)'.format(len(dataset))))\n",
        "\n",
        "    # For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
        "    batch_size = 32\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "                          train_dataset,  # training samples.\n",
        "                          sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "                          batch_size = batch_size # Trains with this batch size.\n",
        "                      )\n",
        "\n",
        "    if split:\n",
        "        # For validation the order doesn't matter, so just read them sequentially.\n",
        "        validation_dataloader = DataLoader(\n",
        "                                    val_dataset, # The validation samples.\n",
        "                                    sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "                                    batch_size = batch_size # Evaluate with this batch size.\n",
        "                                )\n",
        "\n",
        "        return train_dataloader, validation_dataloader\n",
        "\n",
        "    return train_dataloader"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGUqOCtgqGhP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "275e2b55-0b1b-4fb6-df7c-4312701625e1"
      },
      "source": [
        "train_dataloader, validation_dataloader = make_dataloader(input_ids, attention_masks, labels)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6851 training samples\n",
            "762 validation samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bwa6Rts-02-"
      },
      "source": [
        "# 4. Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6TKgyUzPIQc"
      },
      "source": [
        "## 4.1. Define Model: BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFsCTp_mporB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41213b9e-55f2-49fa-a7ab-93651c186f8c"
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification (pretrained BERT model + a single linear classification layer on top)\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "              \"bert-large-uncased\",          # 24-layer BERT large model uncased vocab\n",
        "              num_labels = 2,               # number of output labels (2 for binary classification)\n",
        "              output_attentions = False,    # returns attentions weights?\n",
        "              output_hidden_states = False, # return all hidden-states?\n",
        "        )\n",
        "model.cuda()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 1024)\n",
              "      (token_type_embeddings): Embedding(2, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipilMclbISrq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef101d9-5293-48a9-8656-2cf50f29b7d3"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov  9 01:08:41 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0    30W /  70W |   2177MiB / 15360MiB |     25%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7yujJJkI-Z6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c25f0150-be7b-42a8-e4d6-0666474ecdac"
      },
      "source": [
        "next(model.parameters()).is_cuda"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRWT-D4U_Pvx"
      },
      "source": [
        "## 4.2. Define Optimizer & Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o-VEBobKwHk"
      },
      "source": [
        "For fine-tuning, the authors recommend choosing from following values (from Appendix A.3 of the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf)):\n",
        "\n",
        ">- **Batch size:** 16, **32**  \n",
        "- **Learning rate (Adam):** 5e-5, 3e-5, **2e-5**\n",
        "- **Number of epochs:** **2**, 3, 4\n",
        "\n",
        "*The epsilon parameter `eps = 1e-8` is \"a very small number to prevent any division by zero in the implementation\" (from [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs72DuMODJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb732bf4-3802-415f-854d-2314094c7bd9"
      },
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (not pytorch)- 'W'= 'Weight Decay fix\"\n",
        "optimizer = AdamW(\n",
        "                    model.parameters(),\n",
        "                    lr = 5e-5,         # default\n",
        "                    eps = 1e-8         # default\n",
        "                )"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqfmWwUR_Sox"
      },
      "source": [
        "## 4.3. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cQNvaZ9bnyy"
      },
      "source": [
        "import numpy as np\n",
        "import time, datetime\n",
        "\n",
        "# Helper functions\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''Takes time in seconds and returns a string hh:mm:ss'''\n",
        "\n",
        "    elapsed_rounded = int(round((elapsed)))  # Round to the nearest second\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))  # Format as hh:mm:ss\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    seed_val = 42\n",
        "    random.seed(seed_val)\n",
        "    np.random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaJFXpbeQAaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58b5727a-9cbb-46dc-ffcc-365116ddba7c"
      },
      "source": [
        "len(train_dataloader), len(validation_dataloader)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(215, 24)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-FYdx6nFE_"
      },
      "source": [
        "import random\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def train_BERT(train_dataloader, validation_dataloader, model, optimizer, n_epochs, output_hidden=0):\n",
        "    set_random_seed(seed=42)  # Set seed to make this reproducible.\n",
        "\n",
        "    total_t0 = time.time()   # Measure the total training time for the whole run.\n",
        "    training_stats = []   # Store training and valid loss, valid accuracy, and timings.\n",
        "    hidden_states = []\n",
        "\n",
        "    # lr scheduler\n",
        "    n_batches_train = len(train_dataloader)\n",
        "    scheduler = get_linear_schedule_with_warmup(  optimizer,\n",
        "                                                  num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                  num_training_steps = n_batches_train * n_epochs  )\n",
        "\n",
        "    for epoch_i in range(n_epochs):\n",
        "        # =================== Training =================== #\n",
        "        t0 = time.time()\n",
        "        total_train_loss, total_train_accuracy = 0, 0\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            input_ids, att_mask, labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "            model.zero_grad()\n",
        "            if output_hidden:\n",
        "                loss, logits, h = model(input_ids,\n",
        "                                        token_type_ids=None,\n",
        "                                        attention_mask=att_mask,\n",
        "                                        labels=labels).to_tuple()\n",
        "\n",
        "                h = [layer.detach().cpu().numpy() for layer in h]\n",
        "                if epoch_i == n_epochs - 1: # store the last epoch's hidden states\n",
        "                    hidden_states.append(h[-1]) # only save last layer's h\n",
        "            else:\n",
        "                loss, logits = model(input_ids,\n",
        "                                    token_type_ids=None, # Not required since training on a SINGLE sentence, not a pair\n",
        "                                    attention_mask=att_mask,\n",
        "                                    labels=labels).to_tuple()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            #total_train_loss += loss.detach().cpu().item()\n",
        "            #total_train_loss += float(loss)\n",
        "            total_train_accuracy += flat_accuracy(logits.detach().cpu().numpy(), labels.detach().cpu().numpy())\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()   # Update learning rate\n",
        "\n",
        "        print(\"Epoch: {}/{}\".format((epoch_i+1), n_epochs),\n",
        "              \"  Train loss: {0:.4f}\".format(total_train_loss/n_batches_train),\n",
        "              \"  Train Acc: {0:.4f}\".format(total_train_accuracy/n_batches_train),\n",
        "              \"  ({:})\".format(format_time(time.time() - t0)))\n",
        "\n",
        "        training_stats.append( {'epoch':           epoch_i + 1,\n",
        "                                'Training Loss':   total_train_loss/n_batches_train,\n",
        "                                'Training Acc' :   total_train_accuracy/n_batches_train,\n",
        "                                'Training Time':   format_time(time.time() - t0)} )\n",
        "\n",
        "        if validation_dataloader is not None:\n",
        "            # =================== Validation =================== #\n",
        "            n_batches_valid = len(validation_dataloader)\n",
        "            t0 = time.time()\n",
        "            model.eval()\n",
        "\n",
        "            total_eval_accuracy, total_eval_loss = 0, 0\n",
        "            for batch in validation_dataloader:\n",
        "                v_input_ids, v_att_mask, v_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "                with torch.no_grad():\n",
        "                    if output_hidden:\n",
        "                        loss, logits, val_h = model(v_input_ids,\n",
        "                                                    token_type_ids=None,\n",
        "                                                    attention_mask=v_att_mask,\n",
        "                                                    labels=v_labels).to_tuple()\n",
        "\n",
        "                        val_h = [layer.detach().cpu().numpy() for layer in val_h] # save GPU memory\n",
        "                    else:\n",
        "                        loss, logits = model(v_input_ids,\n",
        "                                             token_type_ids=None,\n",
        "                                             attention_mask=v_att_mask,\n",
        "                                             labels=v_labels).to_tuple()\n",
        "                total_eval_loss += loss.item()\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                label_ids = v_labels.detach().cpu().numpy()\n",
        "                total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "            print(\"  Valid Loss: {0:.4f}\".format(total_eval_loss/n_batches_valid),\n",
        "                  \"  Valid Acc: {0:.4f}\".format(total_eval_accuracy/n_batches_valid),\n",
        "                  \"  ({:})\".format(format_time(time.time()-t0)))\n",
        "\n",
        "            training_stats.append( {'            Valid. Loss':     total_eval_loss/n_batches_valid,\n",
        "                                    'Valid. Acc':   total_eval_accuracy/n_batches_valid,\n",
        "                                    'Validation Time': format_time(time.time()-t0)} )\n",
        "\n",
        "    print(\"\\nTraining complete.\")\n",
        "    print(\"Duration: {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "    if output_hidden:\n",
        "        return training_stats, hidden_states\n",
        "    else:\n",
        "        return training_stats"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1GYtfuaQtUs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d504d1-c017-4efa-c5f8-ef80e9bb349c"
      },
      "source": [
        "training_stats = train_BERT(train_dataloader, validation_dataloader,\n",
        "                            model=model, optimizer=optimizer,\n",
        "                            n_epochs=2)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/2   Train loss: 0.4807   Train Acc: 0.7788   (0:05:40)\n",
            "  Valid Loss: 0.4084   Valid Acc: 0.8240   (0:00:13)\n",
            "Epoch: 2/2   Train loss: 0.3571   Train Acc: 0.8605   (0:05:39)\n",
            "  Valid Loss: 0.4200   Valid Acc: 0.8315   (0:00:13)\n",
            "\n",
            "Training complete.\n",
            "Duration: 0:11:44 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKCevHQXcyMa"
      },
      "source": [
        "## 4.4 Train again w/ ENTIRE data (no validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGsp-RvNOM0i"
      },
      "source": [
        "Now that we observed that 2 epochs are enough in order to prevent overfitting, let's train w/ the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LEVYpTma5AD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9a2f04f-3f71-4bb3-d857-134a61dfdd43"
      },
      "source": [
        "train_dataloader = make_dataloader(input_ids, attention_masks, labels, split=0)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7613 training samples (no validation)\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTLtUpnfbJKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ebfe658-6ead-42be-95e4-4b608c1a3f6c"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "              \"bert-large-uncased\",          # 24-layer BERT large model, w/ uncased vocab\n",
        "              num_labels = 2,               # number of output labels (2 for binary classification)\n",
        "              output_attentions = False,    # Whether the model returns attentions weights.\n",
        "              output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "        )\n",
        "model.cuda()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 1024)\n",
              "      (token_type_embeddings): Embedding(2, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNwLa0lfaZi7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b2dc30b-5902-4483-e4ab-7ed18e681efb"
      },
      "source": [
        "optimizer = AdamW(  model.parameters(), lr = 5e-5, eps = 1e-8)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BcEykI5bpt2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c80c7e8-1bce-4fb1-c9cf-e6b6eb4e6c98"
      },
      "source": [
        "training_stats = train_BERT(train_dataloader, None,\n",
        "                            model=model, optimizer=optimizer,\n",
        "                            n_epochs=2)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/2   Train loss: 0.4614   Train Acc: 0.8047   (0:06:17)\n",
            "Epoch: 2/2   Train loss: 0.3465   Train Acc: 0.8679   (0:06:17)\n",
            "\n",
            "Training complete.\n",
            "Duration: 0:12:33 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyubuJSOzg3"
      },
      "source": [
        "# 5 Predict & Submit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg42jJqqM68F"
      },
      "source": [
        "## 5.1 Test Data Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWe0_JW21MyV"
      },
      "source": [
        "\n",
        "We'll need to apply all of the same steps that we did for the training data to prepare our test data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_V1rDPLWkt5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0176c3c-6a5e-4622-8ba7-11dab18e163e"
      },
      "source": [
        "test_df = pd.read_csv(\"test.csv\")\n",
        "test_sentences = test_df.text.values\n",
        "test_sentences = [\" \".join([word if 'http://' not in word else \"http\" for word in t.split()]) for t in test_sentences]\n",
        "test_sentences = [\" \".join([word for word in t.split() if '@' not in word]) for t in test_sentences]\n",
        "test_encoded_sentences = [tokenizer.encode(s) for s in test_sentences]\n",
        "test_sent_lens = np.array([len(s) for s in test_encoded_sentences])\n",
        "\n",
        "print('# of sentences:', len(test_sentences))\n",
        "print('Max sentence length: ', max(test_sent_lens))\n",
        "print('Avg sentence length: ', np.mean(test_sent_lens))\n",
        "print('Median sentence length: ', np.median(test_sent_lens))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of sentences: 3263\n",
            "Max sentence length:  73\n",
            "Avg sentence length:  23.593931964449894\n",
            "Median sentence length:  23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAN0LZBOOPVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f58612f-6ace-4055-8436-5b58aeeb70e8"
      },
      "source": [
        "encoded_dicts = [tokenizer.encode_plus(  sent,\n",
        "                                         add_special_tokens = True,\n",
        "                                         max_length = 100,\n",
        "                                         pad_to_max_length = True,\n",
        "                                         return_attention_mask = True,\n",
        "                                         return_tensors = 'pt'  ) for sent in test_sentences]\n",
        "input_ids = [d['input_ids'] for d in encoded_dicts]\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = [d['attention_mask'] for d in encoded_dicts]\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "prediction_data = TensorDataset(input_ids, attention_masks)\n",
        "prediction_dataloader = DataLoader(dataset = prediction_data,\n",
        "                                   sampler = SequentialSampler(prediction_data), # doesn't need to be sampled randomly\n",
        "                                   batch_size = 32)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16lctEOyNFik"
      },
      "source": [
        "## 5.2 Predict Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns-iHe7hqA1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fd4417a-4ec7-4878-b3e3-90e803bb5ad5"
      },
      "source": [
        "len(prediction_dataloader)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hba10sXR7Xi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "624107a0-6d29-413d-b4ea-f7c4d2a43bc6"
      },
      "source": [
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "model.eval()\n",
        "predictions, true_labels = [], []\n",
        "for batch in prediction_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask = batch\n",
        "    with torch.no_grad():\n",
        "        logits = model(  b_input_ids,\n",
        "                         token_type_ids=None,\n",
        "                         attention_mask=b_input_mask  ) # no loss, since \"labels\" not provided\n",
        "\n",
        "    logits = logits[0].detach().cpu().numpy() # extract x from [[x]]\n",
        "    predictions.append(logits)\n",
        "print('    DONE.')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 3,263 test sentences...\n",
            "    DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqaLdQM4p0V2"
      },
      "source": [
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFw1qX85gvby"
      },
      "source": [
        "## 5.3 Make Submission File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hYzcna8bfms"
      },
      "source": [
        "submission = pd.DataFrame(test_df.id)\n",
        "submission['target'] = flat_predictions\n",
        "submission.to_csv('submission_11_08_23.csv', index=False)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiO2qrS2cmR_"
      },
      "source": [],
      "execution_count": 53,
      "outputs": []
    }
  ]
}
